\part{Introduction}

\section{Machine learning}

\textbf{Ground truth labels} in 3 types of ML: 
\begin{enumerate}
    \item Supervised learning: labeled data (classification, regression).
    \item Unsupervised learning: unlabeled data, find patterns. 
    \item Reinforcement learning: agent learns by trial/reward.
\end{enumerate}



\part{Supervised Learning}

We are given a \pink{training set} to produce a \pink{hypothesis/model}, and perform \pink{predictions} in the test set (unlabeled examples). 

\section{Data representation}

We are given a \hyperref[training-set]{training set} consisting of \hyperref[input-vector]{inputs} and
corresponding \hyperref[labels]{labels}. 

\subsection{Input vector}\label{input-vector}

\begin{defn}
Input vectors: represent most types of data (images, text, audio, etc.). An input vector $\mathbf{x} \in \mathbb{R}^D$ is written as:
\[
\mathbf{x} =
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_d
\end{bmatrix}
= [x_1 \; x_2 \; \cdots \; x_d]^\top
\]
\end{defn}

\begin{itemize}
    \item Each scalar $x_i$ in $\mathbf{x}$ describes a characteristic of the input. 
\end{itemize}

\subsection{Target output/labels}\label{labels}

\begin{defn}
The ground truth value that we want to predict, denoted as $t \in \mathbb{R}$.
\end{defn}

\subsection{Prediction output}

\begin{defn}
$y \in \mathbb{R}$ as the value generated by our model.
\end{defn}

Types of output: 
\begin{itemize}
    \item Classification: $t \in \{1, \ldots, C\}$ is discrete.
    \item Regression: $t \in \mathbb{R}$ is continuous.
    \item Structured prediction: $t$ is structured (text, images,...).
\end{itemize}

\section{Setup: Training set}

\begin{defn}\label{training-set}
The training data set is a set of $N$ labeled data points:
\[
\{
(\mathbf{x}^{(1)}, t^{(1)}), \; (\mathbf{x}^{(2)}, t^{(2)}), \; \dots, \; (\mathbf{x}^{(N)}, t^{(N)})\}
\]
A \textbf{data point}: a pair of input $\mathbf{x}^{(i)}$ and target output $t^{(i)}$.

\begin{itemize}
    \item Superscript = index of the data point in the dataset.
    \item Subscript = index of the feature in the input vector.
\end{itemize}
\end{defn}

\section{Nearest Neighbors}

Suppose we’re given a novel input vector $\mathbf{x}$ we’d like to classify. The idea is to find the nearest input vector to $\mathbf{x}$ in the training set and copy its label.

\begin{defn}[Nearest neighbours]\label{def:nneighbors}
We’re given a novel input vector $\mathbf{x}$ we’d like to classify.
\begin{enumerate}
    \item Calculate the distance between that  new data point $\mathbf{x}$ and every trainding data point $x^{(i)}$: 
    \[
    \text{distance}(\mathbf{x}^{(i)}, \mathbf{x}).
  \]
  \item Find the training data point $(\mathbf{x}^{(j)}, t^{(j)})$ that is closest to 
  the new data point $\mathbf{x}$:
  \[
    \text{distance}(\mathbf{x}^{(j)}, \mathbf{x}) 
    = \min_{i \in \{1, \dots, N\}} 
    \text{distance}(\mathbf{x}^{(i)}, \mathbf{x}).
  \]
   \item Predict the label $y = t^{(j)}$ for the new data point $\mathbf{x}$.
\end{enumerate}
    
\end{defn}

\subsection{Distance}

\begin{defn}[Euclidean distance ($L^2$ distance)] The $L^2$ norm distance in linear algebra is defined as
\[
    \left\lVert \mathbf{x}^{(i)} - \mathbf{x}^{(j)} \right\rVert_2
= \sqrt{ \sum_{k=1}^{n} \left| x_k^{(i)} - x_k^{(j)} \right|^2 }.
\]
\end{defn}


\begin{defn}[Cosine similarity]
    Measures angle between 2 vectors: 
    \[
\cos(\mathbf{x}^{(i)}, \mathbf{x}^{(j)}) 
= \frac{\mathbf{x}^{(i)} \cdot \mathbf{x}^{(j)}}{\|\mathbf{x}^{(i)}\| \, \|\mathbf{x}^{(j)}\|}.
\]
This comes from the definition of the dot product in Euclidean space. 
\end{defn}

\subsection{Voronoid diagram}
\begin{defn}[2D Voronoid diagram]
    A voronoid diagram is a way to partition the space based on distance to a set of given points. 
    \begin{itemize}
        \item Each point (in the set of given points) is a site. 
        \item After partition the space, we obtain regions. 
        \item The lines separating regions are called edges, formed by perpendicular bisectors between points. 
    \end{itemize}
\end{defn}

\subsection{Decision Boundaries}

\begin{defn}
    The \textbf{decision boundary} is the lines (in 2D) or surface (in higher dimensions) that separates regions in where input space assigned to \textbf{different categories}.  
\end{defn}

\begin{rem}
    The decision boundarys are edges in Voronoid diagram, but not every edges in Voronoid diagram are the decision boundaries. 
\end{rem}



\subsection{K Nearest Neighbors - KNN}

\begin{rem}
    \hyperref[def:nneighbors]{Nearest neighbors} are sensitive to noise or mislabeled data (“class noise”). One solution is to smooth by having k nearest neighbors vote. 
\end{rem}
\begin{eq}
The KNN Algorithm is: 
    \begin{enumerate}
    \item Find $k$ examples $\{(x^{(i)}, t^{(i)})\}_{i=1}^k$ closest to the test instance $x$
    \item Classification output is the majority class:
    \[
        y = \arg\max_{t^{(z)}} \sum_{i=1}^k \mathbb{I}\{t^{(z)} = t^{(i)}\}
    \]
\end{enumerate}

Here, $\mathbb{I}\{\cdot\}$ is the indicator function, which equals 1 if the statement inside is true, and 0 otherwise.

Alternatively, we can write the indicator function using the Kronecker delta function:
\[
    \delta(a, b) =
    \begin{cases}
        1 & \text{if } a = b \\
        0 & \text{otherwise}
    \end{cases}
\]

So the classification rule becomes:
\[
    y = \arg\max_{t^{(z)}} \sum_{i=1}^k \delta(t^{(z)}, t^{(i)})
\]
\end{eq}

\subsection{How to choose $k$ as a hypeparameter?}

\begin{enumerate}
    \item \textbf{Small $k$}
    \begin{enumerate}
        \item Good at capturing fine-grained patterns in the data.
        \item Highly sensitive to noise.
        \item Decision boundary is very complex.
        \item May \textbf{overfit} = high variance.
    \end{enumerate}
    
    \item \textbf{Large $k$}
    \begin{enumerate}
        \item Makes stable predictions by averaging over many examples.
        \item Decision boundary is very smooth.
        \item May \textbf{underfit} = high bias.
    \end{enumerate}
    
    \item \textbf{Balancing $k$}
    \begin{enumerate}
        \item The optimal choice of $k$ depends on the number of data points $n$.
        \item There are nice theoretical properties if:
        \[
            k \rightarrow \infty \quad \text{and} \quad \frac{k}{n} \rightarrow 0
        \]
        \item \textbf{Rule of thumb:}
        \[
            k = \frac{n}{2^{2 + d}}
        \]
        where $d$ is the number of input dimensions (features).
    \end{enumerate}
\end{enumerate}

\subsubsection{Hyperparameter}

\begin{itemize}
    \item \textbf{Parameters}: Learned from data (e.g., weights in a linear model).
    \item \textbf{Hyperparameters}: Set externally; not learned by the model (e.g., $k$ in kNN).
\end{itemize}


\subsubsection{Test Set}

\begin{rem}
    If use test set for choosing $k$, the model is indirectly optimized on test set data. Therefore, test set, which is used to simulate unseen data, must remain untouched until the very end. What should we do else? 
\end{rem}

\subsubsection{Training - Validation - Test Sets}

\begin{eq}
Partition data into 3 sets: 
    \begin{itemize}
    \item \textbf{Training set}: Used to train the model.
    \item \textbf{Validation set}: Used to tune hyperparameters.
    \item \textbf{Test set}: Used to evaluate final performance once, after hyperparameters are chosen.
\end{itemize}

\end{eq}

\section{Curse of Dimensionality}


\begin{problem}
  When your data has too many features (dimensions), the space becomes huge. In high dimensions, all points start to look equally far apart. This makes KNN find it hard to find "true neighbors".  
\end{problem}

\subsection{Intrinsic Dimension}
\begin{defn}[Manifold]
    A manifold is a smooth, lower-dimensional "surface" sitting inside a higher-dimensional space.
\end{defn}
\begin{defn}[Intrindic dimension]
    The dimension of the manifold that data lies on.
\end{defn}

\begin{rem}
    In practice, data often lies on the \textbf{lower-dimensional structure.} Therefore, NN and other learning methods can still work despite the curse of dimensionality. 
\end{rem}


\section{Normalization}
\begin{problem}
    When input features have different ranges (e.g. height in cm, weight in kg), the feature with the larger scale dominates the distance. To solve this, we normalize each dimension to be \textbf{zero mean} and \textbf{unit variance}.
\end{problem}

\begin{eq}
Standardization: 
    $$\Bar{x}_j = \frac{x_j - \mu_j}{\sigma_j}$$
\end{eq}

\section{Computational cost}
\begin{enumerate}
    \item \textbf{Training time:} No work is required (just store the data).
    
    
    \item \textbf{Test time (for each query):}
    \begin{enumerate}
        \item Compute all $N$ distances in $D$ dimensions:
        \[
            O(ND)
        \]
        \item Sort them to find nearest neighbors:
        \[
            O(N \log N)
        \]
    \end{enumerate}
    
    \item \textbf{Problems:}
    \begin{itemize}
        \item If $N$ and $D$ are huge, the computation is very expensive.
        \item Must store the entire dataset $\;\Rightarrow\;$ it is memory heavy.
    \end{itemize}
    
    \item \textbf{Research focus:} Faster nearest-neighbor search (trees, hashing, approximations).
\end{enumerate}



